{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a8h3ojxH7-g"
   },
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "KJBcOgIcAYDQ",
    "outputId": "1c9347e6-644b-4ad5-8560-aa861149c580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-17 07:33:51--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
      "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
      "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 344862509 (329M) [application/x-gzip]\n",
      "Saving to: ‘102flowers.tgz’\n",
      "\n",
      "102flowers.tgz      100%[===================>] 328.89M  99.9MB/s    in 3.3s    \n",
      "\n",
      "2020-05-17 07:33:55 (98.6 MB/s) - ‘102flowers.tgz’ saved [344862509/344862509]\n",
      "\n",
      "--2020-05-17 07:34:06--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
      "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
      "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 502\n",
      "Saving to: ‘imagelabels.mat.1’\n",
      "\n",
      "imagelabels.mat.1   100%[===================>]     502  --.-KB/s    in 0s      \n",
      "\n",
      "2020-05-17 07:34:06 (75.2 MB/s) - ‘imagelabels.mat.1’ saved [502/502]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
    "!tar -xzf 102flowers.tgz\n",
    "!rm 102flowers.tgz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLgruPYzg6k4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdvcEKpHvQye"
   },
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxsMyTjnBPKt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import cv2\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQ1yvdeAIBtc"
   },
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc5k5oMSvV9k"
   },
   "source": [
    "Loading labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsFN7VRBBAsw"
   },
   "outputs": [],
   "source": [
    "img_labels = scipy.io.loadmat(\"imagelabels.mat\")\n",
    "img_labels = img_labels[\"labels\"]\n",
    "img_labels = img_labels[0]\n",
    "for i in range(len(img_labels)):\n",
    "  img_labels[i] = img_labels[i] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acAIhusQvbhP"
   },
   "source": [
    "loading images and converting them tto numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qciPM00rBbwg"
   },
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "dir = \"jpg/\"\n",
    "for imgs in os.listdir(dir):\n",
    "  img_num = int(imgs[7:11])-1\n",
    "  train_y.append(img_labels[img_num])\n",
    "  image = cv2.imread(os.path.join(dir, imgs))\n",
    "  resized = cv2.resize(image, (150,150))\n",
    "  normalized_img = cv2.normalize(resized, None, alpha=0, beta=1, \n",
    "                            norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "  train_x.append(normalized_img)\n",
    "train_x = np.array(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yM3RUQPvlyt"
   },
   "source": [
    "Splitting data in training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsMn7pMwBwck"
   },
   "outputs": [],
   "source": [
    "trainx, valx, trainy, valy = train_test_split(train_x, train_y, test_size=0.15, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rc_eTyNe56oq"
   },
   "source": [
    "acknowledging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kmJ_JS_546vg",
    "outputId": "6957c22b-cb52-4f3f-cf27-0e9d9a24e03d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: ­(6960, 150, 150, 3)\n",
      "No. of Training Dataset Labels: 6960\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Shape: ­{}'.format(trainx.shape))\n",
    "print('No. of Training Dataset Labels: {}'.format(len(trainy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJebyPhr6BFI"
   },
   "source": [
    "reshaping and exploring data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WzRnt3xJ6bg2",
    "outputId": "9ba0865a-a20f-45af-bb22-7977c577f8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: ­(6960, 150, 150, 3)\n",
      "No. of Training Dataset Labels: 6960\n",
      "Test Dataset Shape: (1229, 150, 150, 3)\n",
      "No. of Test Dataset Labels: 1229\n"
     ]
    }
   ],
   "source": [
    "training_images= trainx/255.0\n",
    "test_images=valx/255.0\n",
    "\n",
    "\n",
    "\n",
    "training_images = trainx.reshape((6960,150,150,3))\n",
    "valx = valx.reshape((1229,150,150,3))\n",
    "\n",
    "\n",
    "print('Training Dataset Shape: ­{}'.format(trainx.shape))\n",
    "print('No. of Training Dataset Labels: {}'.format(len(trainy)))\n",
    "print('Test Dataset Shape: {}'.format(valx.shape))\n",
    "print('No. of Test Dataset Labels: {}'.format(len(valy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoX-56u3CG38"
   },
   "outputs": [],
   "source": [
    "trainy = to_categorical(trainy)\n",
    "valy = to_categorical(valy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQY-ye4g7ExT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9z5-KTu7eKw"
   },
   "source": [
    "Important functions for building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iciBxQ4QCNYR"
   },
   "outputs": [],
   "source": [
    "# fn to create weights\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    " # fn to create biases\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "# fn to create convolutional layer\n",
    "def create_convolutional_layer(input,num_input_channels, conv_filter_size, num_filters):  \n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    biases = create_biases(num_filters)\n",
    " \n",
    "    layer = tf.nn.conv2d(input=input, filter=weights,strides=[1, 1, 1, 1],padding='SAME')\n",
    "    layer += biases\n",
    "    layer = tf.nn.relu(layer)  \n",
    "    layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    return layer\n",
    "\n",
    "# fn to create flatten layer\n",
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    " \n",
    "    return layer\n",
    " \n",
    " # fn to create fully connected layers\n",
    "def create_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "    \n",
    "    if use_relu:\n",
    "        layer = tf.add(tf.matmul(input, weights), biases)\n",
    "        layer = tf.nn.relu(layer)\n",
    "    else:\n",
    "        layer = tf.add(tf.matmul(input, weights), biases, name='y_preds')\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7cdy6cKCfTC"
   },
   "outputs": [],
   "source": [
    "## INITIALIZING CONSTANTS\n",
    "x = tf.placeholder(tf.float32, shape=[None, 150,150,3], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, 102], name='y')\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 500\n",
    "KEEP_PROB = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86Z0q3rICu70"
   },
   "outputs": [],
   "source": [
    "## BUILDING CNN\n",
    "\n",
    "# Adding 1st convolutional layer\n",
    "\n",
    "block1_conv1 = create_convolutional_layer(input=x, num_input_channels=3, conv_filter_size=3, num_filters=64)\n",
    "block1_conv2 = create_convolutional_layer(input=block1_conv1, num_input_channels=64,conv_filter_size=3, num_filters=128) \n",
    "batch1 = tf.layers.batch_normalization(block1_conv2) \n",
    "drop1 = tf.nn.dropout(batch1, KEEP_PROB)\n",
    "\n",
    "# Adding 2nd convolutional layer\n",
    "\n",
    "\n",
    "block2_conv1 = create_convolutional_layer(input=drop1, num_input_channels=128, conv_filter_size=3, num_filters=128)\n",
    "block2_conv2 = create_convolutional_layer(input=block2_conv1, num_input_channels=128, conv_filter_size=3, num_filters=256)\n",
    "batch2 = tf.layers.batch_normalization(block2_conv2) \n",
    "drop2 = tf.nn.dropout(batch2, KEEP_PROB)\n",
    "\n",
    "# Adding 3rd convolutional layer\n",
    "\n",
    "\n",
    "block3_conv1 = create_convolutional_layer(input=drop2, num_input_channels=256, conv_filter_size=3, num_filters=256)\n",
    "block3_conv2 = create_convolutional_layer(input=block3_conv1, num_input_channels=256, conv_filter_size=3, num_filters=512)\n",
    "batch3 = tf.layers.batch_normalization(block3_conv2) \n",
    "drop3 = tf.nn.dropout(batch3, KEEP_PROB)\n",
    "\n",
    "\n",
    "\n",
    "layer_flat = create_flatten_layer(drop3)\n",
    "\n",
    "layer_fc1 = create_fc_layer(input=layer_flat, num_inputs=layer_flat.get_shape()[1:4].num_elements(), num_outputs=1024, use_relu=True)\n",
    "batch5 = tf.layers.batch_normalization(layer_fc1)\n",
    "drop5 = tf.nn.dropout(batch5, KEEP_PROB)\n",
    "\n",
    "\n",
    "\n",
    "#output layer\n",
    "y_preds = create_fc_layer(input=drop5, num_inputs=1024, num_outputs=102, use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NTD95inC1US"
   },
   "outputs": [],
   "source": [
    "## CALCULATING COST AND ACCURACY\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_preds, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(y_preds, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJuqSHJ1IQTy"
   },
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bVI2a6mSC_q-",
    "outputId": "99b1d865-3477-4d61-e31e-31e8036efdf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss= 4.5610   Train Accuracy= 0.0360   Valid Loss= 4.5592   Valid Accuracy= 0.0280\n",
      "Epoch 2: Train Loss= 4.5270   Train Accuracy= 0.0260   Valid Loss= 4.5455   Valid Accuracy= 0.0280\n",
      "Epoch 3: Train Loss= 4.5328   Train Accuracy= 0.0420   Valid Loss= 4.5355   Valid Accuracy= 0.0280\n",
      "Epoch 4: Train Loss= 4.5230   Train Accuracy= 0.0200   Valid Loss= 4.5111   Valid Accuracy= 0.0380\n",
      "Epoch 5: Train Loss= 4.4415   Train Accuracy= 0.0520   Valid Loss= 4.4353   Valid Accuracy= 0.0420\n",
      "Epoch 6: Train Loss= 4.3174   Train Accuracy= 0.0400   Valid Loss= 4.3343   Valid Accuracy= 0.0340\n",
      "Epoch 7: Train Loss= 4.2062   Train Accuracy= 0.0400   Valid Loss= 4.2172   Valid Accuracy= 0.0440\n",
      "Epoch 8: Train Loss= 4.0857   Train Accuracy= 0.0680   Valid Loss= 4.1239   Valid Accuracy= 0.0660\n",
      "Epoch 9: Train Loss= 3.9556   Train Accuracy= 0.0760   Valid Loss= 4.0335   Valid Accuracy= 0.0660\n",
      "Epoch 10: Train Loss= 3.8688   Train Accuracy= 0.0920   Valid Loss= 3.9253   Valid Accuracy= 0.0920\n",
      "Epoch 11: Train Loss= 3.6981   Train Accuracy= 0.1020   Valid Loss= 3.8285   Valid Accuracy= 0.1000\n",
      "Epoch 12: Train Loss= 3.6284   Train Accuracy= 0.1080   Valid Loss= 3.6926   Valid Accuracy= 0.1120\n",
      "Epoch 13: Train Loss= 3.4732   Train Accuracy= 0.1360   Valid Loss= 3.4599   Valid Accuracy= 0.1400\n",
      "Epoch 14: Train Loss= 3.3243   Train Accuracy= 0.1740   Valid Loss= 3.3971   Valid Accuracy= 0.1400\n",
      "Epoch 15: Train Loss= 3.2982   Train Accuracy= 0.1780   Valid Loss= 3.3000   Valid Accuracy= 0.1920\n",
      "Epoch 16: Train Loss= 3.1810   Train Accuracy= 0.1980   Valid Loss= 3.2256   Valid Accuracy= 0.1840\n",
      "Epoch 17: Train Loss= 3.0427   Train Accuracy= 0.2140   Valid Loss= 3.1736   Valid Accuracy= 0.2080\n",
      "Epoch 18: Train Loss= 2.9300   Train Accuracy= 0.2460   Valid Loss= 3.0729   Valid Accuracy= 0.2380\n",
      "Epoch 19: Train Loss= 2.9101   Train Accuracy= 0.2360   Valid Loss= 3.0065   Valid Accuracy= 0.2260\n",
      "Epoch 20: Train Loss= 2.8035   Train Accuracy= 0.2740   Valid Loss= 3.0514   Valid Accuracy= 0.2300\n",
      "Epoch 21: Train Loss= 2.7696   Train Accuracy= 0.2800   Valid Loss= 2.8689   Valid Accuracy= 0.2680\n",
      "Epoch 22: Train Loss= 2.7375   Train Accuracy= 0.2800   Valid Loss= 2.8008   Valid Accuracy= 0.2580\n",
      "Epoch 23: Train Loss= 2.5499   Train Accuracy= 0.3040   Valid Loss= 2.8039   Valid Accuracy= 0.2640\n",
      "Epoch 24: Train Loss= 2.5288   Train Accuracy= 0.3300   Valid Loss= 2.7110   Valid Accuracy= 0.3240\n",
      "Epoch 25: Train Loss= 2.5570   Train Accuracy= 0.3220   Valid Loss= 2.6961   Valid Accuracy= 0.2780\n",
      "Epoch 26: Train Loss= 2.3363   Train Accuracy= 0.3600   Valid Loss= 2.6657   Valid Accuracy= 0.3140\n",
      "Epoch 27: Train Loss= 2.3392   Train Accuracy= 0.3860   Valid Loss= 2.7392   Valid Accuracy= 0.2940\n",
      "Epoch 28: Train Loss= 2.3059   Train Accuracy= 0.3740   Valid Loss= 2.5674   Valid Accuracy= 0.3320\n",
      "Epoch 29: Train Loss= 2.2573   Train Accuracy= 0.3820   Valid Loss= 2.6017   Valid Accuracy= 0.3100\n",
      "Epoch 30: Train Loss= 2.3139   Train Accuracy= 0.3800   Valid Loss= 2.6024   Valid Accuracy= 0.3460\n",
      "Epoch 31: Train Loss= 2.1934   Train Accuracy= 0.4120   Valid Loss= 2.5987   Valid Accuracy= 0.3160\n",
      "Epoch 32: Train Loss= 2.1353   Train Accuracy= 0.4120   Valid Loss= 2.5823   Valid Accuracy= 0.3300\n",
      "Epoch 33: Train Loss= 2.0025   Train Accuracy= 0.4520   Valid Loss= 2.5230   Valid Accuracy= 0.3300\n",
      "Epoch 34: Train Loss= 2.0876   Train Accuracy= 0.4480   Valid Loss= 2.4660   Valid Accuracy= 0.3420\n",
      "Epoch 35: Train Loss= 2.0272   Train Accuracy= 0.4460   Valid Loss= 2.5104   Valid Accuracy= 0.3580\n",
      "Epoch 36: Train Loss= 1.9314   Train Accuracy= 0.4580   Valid Loss= 2.4818   Valid Accuracy= 0.3360\n",
      "Epoch 37: Train Loss= 1.9678   Train Accuracy= 0.4400   Valid Loss= 2.4376   Valid Accuracy= 0.3660\n",
      "Epoch 38: Train Loss= 1.8952   Train Accuracy= 0.4660   Valid Loss= 2.4705   Valid Accuracy= 0.3620\n",
      "Epoch 39: Train Loss= 1.7317   Train Accuracy= 0.5040   Valid Loss= 2.3966   Valid Accuracy= 0.3760\n",
      "Epoch 40: Train Loss= 1.7187   Train Accuracy= 0.5000   Valid Loss= 2.3268   Valid Accuracy= 0.3860\n",
      "Epoch 41: Train Loss= 1.6114   Train Accuracy= 0.5380   Valid Loss= 2.3375   Valid Accuracy= 0.3880\n",
      "Epoch 42: Train Loss= 1.4994   Train Accuracy= 0.5840   Valid Loss= 2.3062   Valid Accuracy= 0.3920\n",
      "Epoch 43: Train Loss= 1.5728   Train Accuracy= 0.5580   Valid Loss= 2.3092   Valid Accuracy= 0.3920\n",
      "Epoch 44: Train Loss= 1.4242   Train Accuracy= 0.5640   Valid Loss= 2.3921   Valid Accuracy= 0.4040\n",
      "Epoch 45: Train Loss= 1.6332   Train Accuracy= 0.5440   Valid Loss= 2.4230   Valid Accuracy= 0.3820\n",
      "Epoch 46: Train Loss= 1.5109   Train Accuracy= 0.5660   Valid Loss= 2.3713   Valid Accuracy= 0.3760\n",
      "Epoch 47: Train Loss= 1.3078   Train Accuracy= 0.6100   Valid Loss= 2.2190   Valid Accuracy= 0.4360\n",
      "Epoch 48: Train Loss= 1.2545   Train Accuracy= 0.6140   Valid Loss= 2.3364   Valid Accuracy= 0.4200\n",
      "Epoch 49: Train Loss= 1.2538   Train Accuracy= 0.6320   Valid Loss= 2.2794   Valid Accuracy= 0.4160\n",
      "Epoch 50: Train Loss= 1.4150   Train Accuracy= 0.5860   Valid Loss= 2.4471   Valid Accuracy= 0.3960\n",
      "Epoch 51: Train Loss= 1.2998   Train Accuracy= 0.6260   Valid Loss= 2.4838   Valid Accuracy= 0.4240\n",
      "Epoch 52: Train Loss= 1.1249   Train Accuracy= 0.6560   Valid Loss= 2.4239   Valid Accuracy= 0.3980\n",
      "Epoch 53: Train Loss= 1.2563   Train Accuracy= 0.6020   Valid Loss= 2.4165   Valid Accuracy= 0.4240\n",
      "Epoch 54: Train Loss= 1.2628   Train Accuracy= 0.6140   Valid Loss= 2.5151   Valid Accuracy= 0.3920\n",
      "Epoch 55: Train Loss= 1.2430   Train Accuracy= 0.6220   Valid Loss= 2.5540   Valid Accuracy= 0.3660\n",
      "Epoch 56: Train Loss= 1.3016   Train Accuracy= 0.6260   Valid Loss= 2.5964   Valid Accuracy= 0.3940\n",
      "Epoch 57: Train Loss= 1.1355   Train Accuracy= 0.6600   Valid Loss= 2.3696   Valid Accuracy= 0.4380\n",
      "Epoch 58: Train Loss= 1.1922   Train Accuracy= 0.6200   Valid Loss= 2.6243   Valid Accuracy= 0.3500\n",
      "Epoch 59: Train Loss= 1.2429   Train Accuracy= 0.6320   Valid Loss= 2.5847   Valid Accuracy= 0.3780\n",
      "Epoch 60: Train Loss= 1.1705   Train Accuracy= 0.6620   Valid Loss= 2.5255   Valid Accuracy= 0.4080\n",
      "Epoch 61: Train Loss= 1.0152   Train Accuracy= 0.7080   Valid Loss= 2.2771   Valid Accuracy= 0.4440\n",
      "Epoch 62: Train Loss= 0.9831   Train Accuracy= 0.6860   Valid Loss= 2.3519   Valid Accuracy= 0.4180\n",
      "Epoch 63: Train Loss= 0.9307   Train Accuracy= 0.7240   Valid Loss= 2.5321   Valid Accuracy= 0.4360\n",
      "Epoch 64: Train Loss= 0.8948   Train Accuracy= 0.7320   Valid Loss= 2.4357   Valid Accuracy= 0.4140\n",
      "Epoch 65: Train Loss= 0.8641   Train Accuracy= 0.7560   Valid Loss= 2.4448   Valid Accuracy= 0.4320\n",
      "Epoch 66: Train Loss= 0.8365   Train Accuracy= 0.7360   Valid Loss= 2.5993   Valid Accuracy= 0.4180\n",
      "Epoch 67: Train Loss= 0.9156   Train Accuracy= 0.7380   Valid Loss= 2.5269   Valid Accuracy= 0.4400\n",
      "Epoch 68: Train Loss= 0.7791   Train Accuracy= 0.7700   Valid Loss= 2.5653   Valid Accuracy= 0.4380\n",
      "Epoch 69: Train Loss= 0.7678   Train Accuracy= 0.7500   Valid Loss= 2.4584   Valid Accuracy= 0.4420\n",
      "Epoch 70: Train Loss= 0.7890   Train Accuracy= 0.7560   Valid Loss= 2.6564   Valid Accuracy= 0.4240\n",
      "Epoch 71: Train Loss= 0.7305   Train Accuracy= 0.7560   Valid Loss= 2.5257   Valid Accuracy= 0.4420\n",
      "Epoch 72: Train Loss= 0.6724   Train Accuracy= 0.7880   Valid Loss= 2.5614   Valid Accuracy= 0.4500\n",
      "Epoch 73: Train Loss= 0.6608   Train Accuracy= 0.7800   Valid Loss= 2.4524   Valid Accuracy= 0.4600\n",
      "Epoch 74: Train Loss= 0.6743   Train Accuracy= 0.7820   Valid Loss= 2.6465   Valid Accuracy= 0.4160\n",
      "Epoch 75: Train Loss= 0.6123   Train Accuracy= 0.7840   Valid Loss= 2.5477   Valid Accuracy= 0.4580\n",
      "Epoch 76: Train Loss= 0.5975   Train Accuracy= 0.8060   Valid Loss= 2.5948   Valid Accuracy= 0.4440\n",
      "Epoch 77: Train Loss= 0.5729   Train Accuracy= 0.8320   Valid Loss= 2.5001   Valid Accuracy= 0.4580\n",
      "Epoch 78: Train Loss= 0.5812   Train Accuracy= 0.8060   Valid Loss= 2.5002   Valid Accuracy= 0.4740\n",
      "Epoch 79: Train Loss= 0.5880   Train Accuracy= 0.8180   Valid Loss= 2.6265   Valid Accuracy= 0.4460\n",
      "Epoch 80: Train Loss= 0.5560   Train Accuracy= 0.8380   Valid Loss= 2.6190   Valid Accuracy= 0.4740\n",
      "Epoch 81: Train Loss= 0.5650   Train Accuracy= 0.8100   Valid Loss= 2.7758   Valid Accuracy= 0.4380\n",
      "Epoch 82: Train Loss= 0.5578   Train Accuracy= 0.8020   Valid Loss= 2.6602   Valid Accuracy= 0.4680\n",
      "Epoch 83: Train Loss= 0.4984   Train Accuracy= 0.8340   Valid Loss= 2.6272   Valid Accuracy= 0.4200\n",
      "Epoch 84: Train Loss= 0.5304   Train Accuracy= 0.8360   Valid Loss= 2.7632   Valid Accuracy= 0.4240\n",
      "Epoch 85: Train Loss= 0.5252   Train Accuracy= 0.8320   Valid Loss= 2.6909   Valid Accuracy= 0.4540\n",
      "Epoch 86: Train Loss= 0.5040   Train Accuracy= 0.8420   Valid Loss= 2.5981   Valid Accuracy= 0.4560\n",
      "Epoch 87: Train Loss= 0.4537   Train Accuracy= 0.8600   Valid Loss= 2.6374   Valid Accuracy= 0.4680\n",
      "Epoch 88: Train Loss= 0.3761   Train Accuracy= 0.8620   Valid Loss= 2.6532   Valid Accuracy= 0.4880\n",
      "Epoch 89: Train Loss= 0.4870   Train Accuracy= 0.8340   Valid Loss= 2.7934   Valid Accuracy= 0.4440\n",
      "Epoch 90: Train Loss= 0.4725   Train Accuracy= 0.8320   Valid Loss= 2.8045   Valid Accuracy= 0.4700\n",
      "Epoch 91: Train Loss= 0.4484   Train Accuracy= 0.8700   Valid Loss= 2.7132   Valid Accuracy= 0.4580\n",
      "Epoch 92: Train Loss= 0.3966   Train Accuracy= 0.8720   Valid Loss= 2.9380   Valid Accuracy= 0.4460\n",
      "Epoch 93: Train Loss= 0.3164   Train Accuracy= 0.9020   Valid Loss= 2.7071   Valid Accuracy= 0.4640\n",
      "Epoch 94: Train Loss= 0.3765   Train Accuracy= 0.8820   Valid Loss= 2.8896   Valid Accuracy= 0.4500\n",
      "Epoch 95: Train Loss= 0.4218   Train Accuracy= 0.8600   Valid Loss= 3.0589   Valid Accuracy= 0.4400\n",
      "Epoch 96: Train Loss= 0.3771   Train Accuracy= 0.8760   Valid Loss= 2.6421   Valid Accuracy= 0.4680\n",
      "Epoch 97: Train Loss= 0.3731   Train Accuracy= 0.8860   Valid Loss= 2.8042   Valid Accuracy= 0.4360\n",
      "Epoch 98: Train Loss= 0.3305   Train Accuracy= 0.8900   Valid Loss= 2.9577   Valid Accuracy= 0.4760\n",
      "Epoch 99: Train Loss= 0.4255   Train Accuracy= 0.8540   Valid Loss= 3.0951   Valid Accuracy= 0.4440\n",
      "Epoch 100: Train Loss= 0.3030   Train Accuracy= 0.9040   Valid Loss= 2.7476   Valid Accuracy= 0.4940\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /model5/saved_model.pb\n",
      "--- MODEL SAVED ---\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "      for batch in range(int(len(trainx)/BATCH_SIZE)):\n",
    "        batch_x = trainx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainx))]\n",
    "        batch_y = trainy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainy))]\n",
    "\n",
    "        opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "      for batch in range(int(len(valx)/BATCH_SIZE)):\n",
    "        val_batch_x = valx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valx))]\n",
    "        val_batch_y = valy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valy))]\n",
    "\n",
    "        val_loss, val_acc= sess.run([cost, accuracy], feed_dict={x: val_batch_x, y: val_batch_y})\n",
    "        \n",
    "      print(\"Epoch \"+str(epoch+1)+\": Train Loss= \"+\"{:.4f}\".format(loss)+\"   Train Accuracy= \" +  \"{:.4f}\".format(acc)+\n",
    "              \"   Valid Loss= \"+\"{:.4f}\".format(val_loss)+\"   Valid Accuracy= \" + \"{:.4f}\".format(val_acc))\n",
    "\n",
    "    ## SAVING THE MODEL\n",
    "    os.mkdir('/model5')\n",
    "    tf.saved_model.simple_save(sess, '/model5', inputs={\"x\": x}, outputs={\"y_preds\": y_preds})\n",
    "    print('--- MODEL SAVED ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIaHEj5rKKh2"
   },
   "source": [
    "#### Loading saved model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmr58WcIEn8y"
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.saved_model.loader.load(sess, [\"serve\"], '/model5')\n",
    "\n",
    "        x = graph.get_tensor_by_name('x:0')\n",
    "        y_preds = graph.get_tensor_by_name('y_preds:0')\n",
    "        \n",
    "        y_true = []\n",
    "        preds = []\n",
    "        for batch in range(int(len(valx)/BATCH_SIZE)):\n",
    "          batch_x = valx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valx))]\n",
    "          batch_y = valy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valy))]\n",
    "\n",
    "          y_true.append(batch_y)\n",
    "          preds.append(sess.run(y_preds, feed_dict={x: batch_x}))\n",
    "\n",
    "        y_true = np.stack(np.array(y_true), axis=0)\n",
    "        preds = np.stack(np.array(preds), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbY8n6jKFjVK"
   },
   "outputs": [],
   "source": [
    "# Calculte loss and accuracy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = tf.cast(preds, tf.float32), \n",
    "                                                              labels=tf.cast(y_true, tf.float32)))\n",
    "correct = tf.equal(np.argmax(preds, axis=2), np.argmax(y_true, axis=2))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8TQlXGdnFqtK",
    "outputId": "c74b21a3-69f8-42b2-8dc0-8dfa297b2922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.5510135\n",
      "Accuracy : 0.416\n"
     ]
    }
   ],
   "source": [
    "# Printing results\n",
    "with tf.Session() as sess:\n",
    "    print('Loss :',loss.eval())\n",
    "    print('Accuracy :', accuracy.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Flowers_with_Tensorflow (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
